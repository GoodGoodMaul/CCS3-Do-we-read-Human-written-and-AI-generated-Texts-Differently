"""
Build a Qwen3-friendly Yes/No dataset that detects whether a passage was AI-generated.
"""

import csv
import json
from collections import Counter
from pathlib import Path
from typing import Dict, List

try:
    from datasets import Dataset, DatasetDict
except ModuleNotFoundError as exc:
    raise SystemExit(
        "datasets dependency is missing. Install with `pip install -U datasets fsspec`."
    ) from exc

try:
    import yaml
except ModuleNotFoundError as exc:
    raise SystemExit("PyYAML dependency is missing. Install with `pip install -U pyyaml`.") from exc

PROJECT_ROOT = Path(__file__).resolve().parents[2]
CONFIG_PATH = PROJECT_ROOT / "configs" / "base.yaml"
DATASET_NAME = "text_ai_detection"

PROMPT_TEMPLATE = (
    "You are an AI-generation detector. Read the passage and answer only 'Yes' or 'No'.\n"
    "Question: Was the following text generated by an AI system?\n\n"
    "{text}"
)


def load_base_config(config_path: Path) -> Dict:
    with config_path.open(encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def resolve_path(config: Dict, key: str, default: str) -> Path:
    raw_path = config.get(key, default)
    path = Path(raw_path)
    return path if path.is_absolute() else PROJECT_ROOT / path


def build_records(csv_path: Path) -> List[Dict]:
    records: List[Dict] = []
    with csv_path.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            text = (row.get("Text") or "").strip()
            source = (row.get("Source") or "").strip()
            split = (row.get("Split") or "").strip()
            label = "Yes" if source.lower() == "ai" else "No"
            user_prompt = PROMPT_TEMPLATE.format(text=text)

            records.append(
                {
                    "split": split,
                    "source": source,
                    "text": text,
                    "user_prompt": user_prompt,
                    "assistant": label,
                    "messages": [
                        {"role": "user", "content": user_prompt},
                        {"role": "assistant", "content": label},
                    ],
                }
            )
    return records


def build_split_datasets(records: List[Dict]) -> DatasetDict:
    splits = sorted({record["split"] for record in records})
    split_map = {
        split: Dataset.from_list([record for record in records if record["split"] == split])
        for split in splits
    }
    return DatasetDict(split_map)


def save_first_sample(dataset: Dataset, output_path: Path) -> None:
    if len(dataset) == 0:
        raise ValueError("Dataset is empty; cannot save the first sample.")
    output_path.write_text(json.dumps(dataset[0], ensure_ascii=False, indent=2), encoding="utf-8")


def main() -> None:
    config = load_base_config(CONFIG_PATH)
    input_dir = resolve_path(config, "input_data_path", "dataset/raw_data")
    output_dir = resolve_path(config, "output_data_path", "dataset/processed_data")

    raw_data_path = input_dir / "data.csv"
    dataset_text_dir = output_dir / "dataset_text"
    dataset_text_dir.mkdir(parents=True, exist_ok=True)

    records = build_records(raw_data_path)
    dataset = Dataset.from_list(records)
    split_datasets = build_split_datasets(records)

    splits_out_path = dataset_text_dir / f"{DATASET_NAME}_splits"
    first_sample_path = dataset_text_dir / f"{DATASET_NAME}_first_sample.json"

    split_datasets.save_to_disk(splits_out_path)
    save_first_sample(dataset, first_sample_path)

    split_counts = Counter(record["split"] for record in records)
    print(f"Built dataset with {len(dataset)} samples from {raw_data_path}.")
    print("Split sizes:", dict(split_counts))
    for split_name, split_ds in split_datasets.items():
        print(f"{split_name} subset: {len(split_ds)} samples")
    print(f"Saved split datasets to {splits_out_path}")
    print(f"First sample saved to {first_sample_path}")


if __name__ == "__main__":
    main()
