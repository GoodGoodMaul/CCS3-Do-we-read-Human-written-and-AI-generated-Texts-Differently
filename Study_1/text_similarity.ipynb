{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24500e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric               | AI Mean    | Human Mean | T-value    | P-value   \n",
      "----------------------------------------------------------------------\n",
      "Word_Count           | 349.38     | 337.12     | 3.326      | 0.013     \n",
      "Char_Count           | 559.00     | 562.38     | -0.944     | 0.377     \n",
      "Avg_Sent_Len         | 31.99      | 33.36      | -1.587     | 0.157     \n",
      "TTR                  | 0.59       | 0.54       | 3.933      | 0.006     \n",
      "\n",
      "Detailed Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Word_Count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Char_Count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Avg_Sent_Len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">TTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>AI</th>\n",
       "      <th>Human</th>\n",
       "      <th>AI</th>\n",
       "      <th>Human</th>\n",
       "      <th>AI</th>\n",
       "      <th>Human</th>\n",
       "      <th>AI</th>\n",
       "      <th>Human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text_Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Argument</th>\n",
       "      <td>370.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>26.761905</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Description</th>\n",
       "      <td>367.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>26.652174</td>\n",
       "      <td>0.694823</td>\n",
       "      <td>0.584046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dialogue</th>\n",
       "      <td>364.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>28.555556</td>\n",
       "      <td>29.111111</td>\n",
       "      <td>0.445055</td>\n",
       "      <td>0.393162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction</th>\n",
       "      <td>336.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>25.105263</td>\n",
       "      <td>24.150000</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keywords</th>\n",
       "      <td>429.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>35.600000</td>\n",
       "      <td>39.111111</td>\n",
       "      <td>0.578089</td>\n",
       "      <td>0.529703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poetry</th>\n",
       "      <td>128.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>28.142857</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.679688</td>\n",
       "      <td>0.674242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summary</th>\n",
       "      <td>356.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>39.875000</td>\n",
       "      <td>44.928571</td>\n",
       "      <td>0.648876</td>\n",
       "      <td>0.586103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Synopsis</th>\n",
       "      <td>445.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>47.312500</td>\n",
       "      <td>45.176471</td>\n",
       "      <td>0.577528</td>\n",
       "      <td>0.530093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word_Count        Char_Count        Avg_Sent_Len             \\\n",
       "Source              AI  Human         AI  Human           AI      Human   \n",
       "Text_Type                                                                 \n",
       "Argument         370.0  360.0      562.0  561.0    26.761905  26.714286   \n",
       "Description      367.0  351.0      615.0  613.0    24.600000  26.652174   \n",
       "Dialogue         364.0  351.0      514.0  524.0    28.555556  29.111111   \n",
       "Fiction          336.0  336.0      477.0  483.0    25.105263  24.150000   \n",
       "Keywords         429.0  404.0      712.0  704.0    35.600000  39.111111   \n",
       "Poetry           128.0  132.0      197.0  217.0    28.142857  31.000000   \n",
       "Summary          356.0  331.0      638.0  629.0    39.875000  44.928571   \n",
       "Synopsis         445.0  432.0      757.0  768.0    47.312500  45.176471   \n",
       "\n",
       "                  TTR            \n",
       "Source             AI     Human  \n",
       "Text_Type                        \n",
       "Argument     0.594595  0.555556  \n",
       "Description  0.694823  0.584046  \n",
       "Dialogue     0.445055  0.393162  \n",
       "Fiction      0.464286  0.458333  \n",
       "Keywords     0.578089  0.529703  \n",
       "Poetry       0.679688  0.674242  \n",
       "Summary      0.648876  0.586103  \n",
       "Synopsis     0.577528  0.530093  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84428e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文本数据...\n",
      "\n",
      "===============================================================================================\n",
      "Metric               | AI Mean (SD)         | Human Mean (SD)      | W-stat   | p-value  | Cohen's d\n",
      "===============================================================================================\n",
      "Word_Count           | 286.38 (85.65)       | 275.88 (78.84)       | 1.0      | 0.027    | 1.178   \n",
      "Char_Count           | 556.62 (173.01)      | 554.88 (172.57)      | 6.0      | 0.343    | 0.282   \n",
      "Log_TTR              | 0.93 (0.03)          | 0.92 (0.03)          | 0.0      | 0.008    | 1.539   \n",
      "Lexical_Density      | 0.63 (0.07)          | 0.63 (0.09)          | 17.0     | 0.945    | -0.104  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmy/anaconda3/envs/adl/lib/python3.11/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/Users/jmy/anaconda3/envs/adl/lib/python3.11/site-packages/scipy/stats/_morestats.py:3428: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/Users/jmy/anaconda3/envs/adl/lib/python3.11/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/Users/jmy/anaconda3/envs/adl/lib/python3.11/site-packages/scipy/stats/_morestats.py:3428: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import jieba.posseg as pseg\n",
    "import string\n",
    "\n",
    "cn_punctuation = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞？＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "en_punctuation = string.punctuation\n",
    "all_punctuation = set(cn_punctuation + en_punctuation)\n",
    "\n",
    "def analyze_text(text_content):\n",
    "    \"\"\"\n",
    "    输入：分词后的字符串（词与词之间用空格隔开）\n",
    "    输出：Word_Count, Char_Count, Log_TTR, Lexical_Density\n",
    "    \"\"\"\n",
    "    text_content = text_content.strip()\n",
    "    \n",
    "    raw_tokens = [w for w in text_content.split(' ') if w.strip() != '']\n",
    "    \n",
    "\n",
    "    valid_words = [w for w in raw_tokens if w.strip() not in all_punctuation]\n",
    "    word_count = len(valid_words)\n",
    "    \n",
    "\n",
    "    clean_text_for_char = re.sub(r'\\s+', '', text_content)\n",
    "    char_count = len(clean_text_for_char)\n",
    "    \n",
    "    unique_words = set(valid_words)\n",
    "    num_unique = len(unique_words)\n",
    "    \n",
    "    if word_count > 1:\n",
    "        log_ttr = math.log(num_unique) / math.log(word_count)\n",
    "    else:\n",
    "        log_ttr = 0\n",
    "    \n",
    "    content_word_count = 0\n",
    "    \n",
    "    for w in valid_words:\n",
    "        words_flags = pseg.cut(w) \n",
    "        for word, flag in words_flags:\n",
    "            if flag.startswith(('n', 'v', 'a', 'd')):\n",
    "                content_word_count += 1\n",
    "                \n",
    "    lexical_density = content_word_count / word_count if word_count > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Word_Count': word_count,\n",
    "        'Char_Count': char_count,\n",
    "        'Log_TTR': log_ttr,\n",
    "        'Lexical_Density': lexical_density\n",
    "    }\n",
    "\n",
    "\n",
    "file_dir = './data'  \n",
    "if not os.path.exists(file_dir):\n",
    "    print(f\"Warning: 目录 {file_dir} 不存在。\")\n",
    "    files = []\n",
    "else:\n",
    "    files = [f for f in os.listdir(file_dir) if f.startswith('[MD]') and f.endswith('.txt')]\n",
    "\n",
    "data_list = []\n",
    "text_types = [\n",
    "    'Argument', 'Description', 'Dialogue', 'Fiction', \n",
    "    'Keywords', 'Poetry', 'Summary', 'Synopsis'\n",
    "]\n",
    "\n",
    "print(\"正在处理文本数据...\")\n",
    "\n",
    "for t_type in text_types:\n",
    "    file_ai = f\"[MD]{t_type}_AI.txt\"\n",
    "    file_human = f\"[MD]{t_type}_Human.txt\"\n",
    "    \n",
    "    if file_ai in files and file_human in files:\n",
    "        try:\n",
    "            with open(os.path.join(file_dir, file_ai), 'r', encoding='utf-8') as f:\n",
    "                stats_ai = analyze_text(f.read())\n",
    "            with open(os.path.join(file_dir, file_human), 'r', encoding='utf-8') as f:\n",
    "                stats_human = analyze_text(f.read())\n",
    "                \n",
    "            data_list.append({'Text_Type': t_type, 'Source': 'AI', **stats_ai})\n",
    "            data_list.append({'Text_Type': t_type, 'Source': 'Human', **stats_human})\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {t_type} 时出错: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if not data_list:\n",
    "    print(\"错误：没有生成数据，请检查文件名是否匹配。\")\n",
    "else:\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    df_ai = df[df['Source'] == 'AI'].set_index('Text_Type')\n",
    "    df_human = df[df['Source'] == 'Human'].set_index('Text_Type')\n",
    "    \n",
    "    metrics = ['Word_Count', 'Char_Count', 'Log_TTR', 'Lexical_Density']\n",
    "    comparison_results = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*95)\n",
    "    col_name_d = \"Cohen's d\"\n",
    "    print(f\"{'Metric':<20} | {'AI Mean (SD)':<20} | {'Human Mean (SD)':<20} | {'W-stat':<8} | {'p-value':<8} | {col_name_d:<8}\")\n",
    "    print(\"=\"*95)\n",
    "\n",
    "    for metric in metrics:\n",
    "        ai_values = df_ai[metric]\n",
    "        human_values = df_human[metric]\n",
    "        \n",
    "        differences = ai_values - human_values\n",
    "        \n",
    "        if np.all(differences == 0):\n",
    "            w_stat, p_val = 0, 1.0\n",
    "        else:\n",
    "            w_stat, p_val = stats.wilcoxon(ai_values, human_values, alternative='two-sided')\n",
    "        \n",
    "        mean_diff = np.mean(differences)\n",
    "        std_diff = np.std(differences, ddof=1)\n",
    "        \n",
    "        if std_diff == 0:\n",
    "            cohens_d = 0\n",
    "        else:\n",
    "            cohens_d = mean_diff / std_diff\n",
    "\n",
    "        ai_desc = f\"{np.mean(ai_values):.2f} ({np.std(ai_values, ddof=1):.2f})\"\n",
    "        human_desc = f\"{np.mean(human_values):.2f} ({np.std(human_values, ddof=1):.2f})\"\n",
    "        \n",
    "        print(f\"{metric:<20} | {ai_desc:<20} | {human_desc:<20} | {w_stat:<8.1f} | {p_val:<8.3f} | {cohens_d:<8.3f}\")\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Metric': metric,\n",
    "            'AI_Mean': np.mean(ai_values),\n",
    "            'AI_SD': np.std(ai_values, ddof=1),\n",
    "            'Human_Mean': np.mean(human_values),\n",
    "            'Human_SD': np.std(human_values, ddof=1),\n",
    "            'Wilcoxon_W': w_stat,\n",
    "            'p_value': p_val,\n",
    "            'Cohens_d': cohens_d\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44752e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "具体文本量统计 (Quantity Statistics by Text Type)\n",
      "===============================================================================================\n",
      "Source              AI      Human         AI      Human\n",
      "            Word_Count Word_Count Char_Count Char_Count\n",
      "Text_Type                                              \n",
      "Argument           305        294        557        556\n",
      "Description        305        294        610        607\n",
      "Dialogue           293        280        514        515\n",
      "Fiction            262        262        477        477\n",
      "Keywords           368        345        709        700\n",
      "Poetry              95         99        196        196\n",
      "Summary            295        276        636        625\n",
      "Synopsis           368        357        754        763\n",
      "TOTAL             2291       2207       4453       4439\n",
      "-----------------------------------------------------------------------------------------------\n",
      "注：上表展示了每种文本类型对应的具体 词数(Word) 和 字符数(Char)。\n",
      "总计处理文件对数: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(\"具体文本量统计 (Quantity Statistics by Text Type)\")\n",
    "print(\"=\"*95)\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "\n",
    "    pivot_df = df.pivot_table(\n",
    "        index='Text_Type', \n",
    "        columns='Source', \n",
    "        values=['Word_Count', 'Char_Count'],\n",
    "        aggfunc='sum' \n",
    "    )\n",
    "\n",
    "    pivot_df = pivot_df.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "\n",
    "    pivot_df[('Diff', 'Words')] = pivot_df[('AI', 'Word_Count')] - pivot_df[('Human', 'Word_Count')]\n",
    "    pivot_df[('Diff', 'Chars')] = pivot_df[('AI', 'Char_Count')] - pivot_df[('Human', 'Char_Count')]\n",
    "\n",
    "    total_row = pivot_df.sum(numeric_only=True)\n",
    "    pivot_df.loc['TOTAL'] = total_row\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    output_cols = [\n",
    "        ('AI', 'Word_Count'), ('Human', 'Word_Count'), \n",
    "        ('AI', 'Char_Count'), ('Human', 'Char_Count')\n",
    "    ]\n",
    "    \n",
    "    print(pivot_df[output_cols].astype(int))\n",
    "\n",
    "    print(\"-\" * 95)\n",
    "    print(\"注：上表展示了每种文本类型对应的具体 词数(Word) 和 字符数(Char)。\")\n",
    "    print(f\"总计处理文件对数: {len(df)//2}\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"DataFrame 为空，无法进行统计。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
